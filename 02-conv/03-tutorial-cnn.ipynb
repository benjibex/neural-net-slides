{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start an interactive session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x1723a88ec88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some basic convolutional neural network operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Start with loading MNIST Dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist\", one_hot=True)\n",
    "n_input = 28*28                    # MNIST image feature size\n",
    "\n",
    "# Let's pick the first several data points for illustration\n",
    "batch_size = 128\n",
    "X_train, y_train = mnist.train.next_batch(batch_size) # get 64 data points\n",
    "X_valid, y_valid = mnist.train.next_batch(500) # get 64 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is digit 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17241a69b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1723f29e3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indx = 9\n",
    "print(\"Image is digit {}\".format(np.argwhere(y_train[indx]==1).flatten()[0]))\n",
    "plt.imshow(X_train[indx].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders for the input\n",
    "# tensor shape as (?, 784), set the first dimension to be unknown, \n",
    "# as in actual training, batch size is sometimes unknown until runtime\n",
    "x = tf.placeholder(tf.float32, [None, n_input]) \n",
    "y = tf.placeholder(tf.int32, (None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def my_conv_model(x):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # reshape to an image\n",
    "    x_reshape = tf.reshape(x, shape=[-1, 28, 28, 1])    \n",
    "    \n",
    "    # layer 1: convolution, input 28x28x1 output: 28x28x6\n",
    "    conv1_W = tf.Variable(tf.random_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x_reshape, conv1_W, strides=[1, 1, 1, 1], padding='SAME') + conv1_b\n",
    "    \n",
    "    # activation\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Flatten. Input: 14x14x6 output = 1176\n",
    "    fc0   = flatten(conv1)\n",
    "    \n",
    "    # Layer 2: Fully connected. Input: 1176, output: 10\n",
    "    fc1_W = tf.Variable(tf.random_normal(shape=(1176, 10), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(10))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    logits = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input]) \n",
    "x_reshape = tf.reshape(x, shape=[-1, 28, 28, 1]) # reshape to (?, 28, 28, 1)\n",
    "y = tf.placeholder(tf.int32, (None, 10))\n",
    "#one_hot_y = tf.one_hot(y, 10) # 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = my_conv_model(x_reshape)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.864\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.890\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=3\n",
    "BATCHES_PER_EPOCH=100\n",
    "BATCH_SIZE=128\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        for _ in range(BATCHES_PER_EPOCH):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)        \n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    print(\"Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
